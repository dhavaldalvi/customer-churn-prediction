{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '')))\n",
    "from src.customer_churn_prediction.utils import DropColumnsTransformer\n",
    "from src.customer_churn_prediction.utils import remove_numerical_outliers\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Exited'] = df['Exited'].replace({1: 'Yes', 0: 'No'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_numerical_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Exited'] = df['Exited'].replace({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(['Exited'], axis=1), df['Exited']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['Geography', 'Gender']\n",
    "numerical_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "dropped_cols = ['RowNumber', 'CustomerId', 'Surname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = DropColumnsTransformer(dropped_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('drop', columns_to_drop, dropped_cols),\n",
    "        ('num', StandardScaler(), numerical_cols),  # Apply StandardScaler to numerical columns\n",
    "        ('cat', OneHotEncoder(), categorical_cols)      # Apply OneHotEncoder to nominal columns\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for KNN...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Running GridSearchCV for SVM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Running GridSearchCV for DecisionTree...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Running GridSearchCV for RandomForest...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Running GridSearchCV for NaiveBayes...\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Running GridSearchCV for GradientBoosting...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Evaluating best model: LogisticRegression with hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81      1524\n",
      "           1       0.41      0.74      0.53       390\n",
      "\n",
      "    accuracy                           0.73      1914\n",
      "   macro avg       0.66      0.73      0.67      1914\n",
      "weighted avg       0.81      0.73      0.75      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: KNN with hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      1524\n",
      "           1       0.70      0.39      0.50       390\n",
      "\n",
      "    accuracy                           0.84      1914\n",
      "   macro avg       0.78      0.67      0.70      1914\n",
      "weighted avg       0.83      0.84      0.82      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: SVM with hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.79      0.85      1524\n",
      "           1       0.47      0.75      0.58       390\n",
      "\n",
      "    accuracy                           0.78      1914\n",
      "   macro avg       0.70      0.77      0.72      1914\n",
      "weighted avg       0.83      0.78      0.80      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: DecisionTree with hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88      1524\n",
      "           1       0.52      0.48      0.50       390\n",
      "\n",
      "    accuracy                           0.80      1914\n",
      "   macro avg       0.69      0.68      0.69      1914\n",
      "weighted avg       0.80      0.80      0.80      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: RandomForest with hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91      1524\n",
      "           1       0.74      0.44      0.55       390\n",
      "\n",
      "    accuracy                           0.86      1914\n",
      "   macro avg       0.81      0.70      0.73      1914\n",
      "weighted avg       0.84      0.86      0.84      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: NaiveBayes with hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88      1524\n",
      "           1       0.52      0.38      0.44       390\n",
      "\n",
      "    accuracy                           0.80      1914\n",
      "   macro avg       0.68      0.65      0.66      1914\n",
      "weighted avg       0.78      0.80      0.79      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: GradientBoosting with hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.92      1524\n",
      "           1       0.76      0.46      0.58       390\n",
      "\n",
      "    accuracy                           0.86      1914\n",
      "   macro avg       0.82      0.71      0.75      1914\n",
      "weighted avg       0.85      0.86      0.85      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'NaiveBayes': GaussianNB(),\n",
    "    'GradientBoosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'class_weight':['balanced'],\n",
    "        'max_iter':[500]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight':['balanced']\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight':['balanced']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'class_weight':['balanced']\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        # Gaussian Naive Bayes doesn't require much tuning, but we can tweak priors if needed.\n",
    "        'priors': [None, [0.3, 0.7]],\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Running GridSearchCV on each model\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running GridSearchCV for {model_name}...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_preprocessed, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best models\n",
    "for model_name, model in best_models.items():\n",
    "    print(f\"\\nEvaluating best model: {model_name} with hyperparameters: {grid_search.best_params_}\")\n",
    "    y_pred = model.predict(preprocessor.transform(X_test))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the target set is imbalanced, we will try to test this models with oversampled minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the minority class to balance the target\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running models after oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Running GridSearchCV for KNN...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Running GridSearchCV for SVM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Running GridSearchCV for DecisionTree...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Running GridSearchCV for RandomForest...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Running GridSearchCV for NaiveBayes...\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Running GridSearchCV for GradientBoosting...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Evaluating best model: LogisticRegression with hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.73      0.81      1524\n",
      "           1       0.41      0.73      0.53       390\n",
      "\n",
      "    accuracy                           0.73      1914\n",
      "   macro avg       0.66      0.73      0.67      1914\n",
      "weighted avg       0.81      0.73      0.75      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: KNN with hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.85      1524\n",
      "           1       0.46      0.60      0.52       390\n",
      "\n",
      "    accuracy                           0.77      1914\n",
      "   macro avg       0.67      0.71      0.69      1914\n",
      "weighted avg       0.80      0.77      0.78      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: SVM with hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.85      1524\n",
      "           1       0.47      0.64      0.54       390\n",
      "\n",
      "    accuracy                           0.78      1914\n",
      "   macro avg       0.68      0.73      0.70      1914\n",
      "weighted avg       0.81      0.78      0.79      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: DecisionTree with hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.85      1524\n",
      "           1       0.43      0.54      0.48       390\n",
      "\n",
      "    accuracy                           0.76      1914\n",
      "   macro avg       0.65      0.68      0.66      1914\n",
      "weighted avg       0.78      0.76      0.77      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: RandomForest with hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      1524\n",
      "           1       0.58      0.57      0.57       390\n",
      "\n",
      "    accuracy                           0.83      1914\n",
      "   macro avg       0.74      0.73      0.73      1914\n",
      "weighted avg       0.83      0.83      0.83      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: NaiveBayes with hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.72      0.80      1524\n",
      "           1       0.40      0.71      0.51       390\n",
      "\n",
      "    accuracy                           0.72      1914\n",
      "   macro avg       0.65      0.72      0.66      1914\n",
      "weighted avg       0.80      0.72      0.74      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Evaluating best model: GradientBoosting with hyperparameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      1524\n",
      "           1       0.63      0.52      0.57       390\n",
      "\n",
      "    accuracy                           0.84      1914\n",
      "   macro avg       0.76      0.72      0.74      1914\n",
      "weighted avg       0.83      0.84      0.84      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'NaiveBayes': GaussianNB(),\n",
    "    'GradientBoosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'class_weight':['balanced'],\n",
    "        'max_iter':[500]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight':['balanced']\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight':['balanced']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'class_weight':['balanced']\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        # Gaussian Naive Bayes doesn't require much tuning, but we can tweak priors if needed.\n",
    "        'priors': [None, [0.3, 0.7]],\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Running GridSearchCV on each model\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running GridSearchCV for {model_name}...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best models\n",
    "for model_name, model in best_models.items():\n",
    "    print(f\"\\nEvaluating best model: {model_name} with hyperparameters: {grid_search.best_params_}\")\n",
    "    y_pred = model.predict(preprocessor.transform(X_test))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for XGBClassifier...\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "\n",
      "Evaluating best model: XGBClassifier with hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      1524\n",
      "           1       0.66      0.56      0.60       390\n",
      "\n",
      "    accuracy                           0.85      1914\n",
      "   macro avg       0.78      0.74      0.76      1914\n",
      "weighted avg       0.84      0.85      0.85      1914\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = {\n",
    "    'XGBClassifier': XGBClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'XGBClassifier':{\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Running GridSearchCV on each model\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running GridSearchCV for {model_name}...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best models\n",
    "for model_name, model in best_models.items():\n",
    "    print(f\"\\nEvaluating best model: {model_name} with hyperparameters: {grid_search.best_params_}\")\n",
    "    y_pred = model.predict(preprocessor.transform(X_test))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost performs well with hyperparameters {colsample_bytree = 0.8, gamma = 0, learning_rate = 0.1, max_depth = 7, n_estimators = 200, subsample = 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
